## âš™ï¸ Environment Setup

- ğŸ§  **InternVL**, **LLaVA-Next-7B**, and **Video-LLaVA-7B**  
  â†’ Use **`env1.yaml`** to replicate the target conda environment.

- ğŸ¦‰ **mPLUG-Owl3-7B**  
  â†’ Use **`env2.yaml`**, as it requires a specific version of the `transformers` package.

---

## ğŸ¯ Project Overview

This repository is built to **evaluate the reasoning and grounding capabilities** of  
**Video-Language Models (VLMs)**.  

- ğŸ“ Focus: **Moderate-sized models (< 10B parameters)**  
- ğŸ¬ Domain: **Video-based understanding and reasoning**  
- âš¡ Goal: Benchmark **grounded reasoning** performance efficiently

---

## ğŸ“‚ Repository Structure

Each modelâ€™s directory contains:
- ğŸ§© Original scripts from the respective source repositories  
- ğŸ”§ **Modified and optimized versions** tailored to our evaluation setup  
- ğŸ““ Jupyter notebooks for testing and visualization

---

## ğŸš€ Quick Note

Ensure that you activate the correct conda environment before running any notebook:  

```bash
conda env create -f env1.yaml   # For InternVL / LLaVA-Next / Video-LLaVA
conda env create -f env2.yaml   # For mPLUG-Owl3
